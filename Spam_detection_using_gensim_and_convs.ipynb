{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spam detection using gensim and convs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMFq93UzWC7RoiiwOSM5wd+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/narsym/deep-learning-with-tensorflow-2.0/blob/master/Spam_detection_using_gensim_and_convs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eas9jpYSjABU",
        "colab_type": "text"
      },
      "source": [
        "Imports "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21SjkCjjhyqU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "import gensim.downloader as api\n",
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igQRtGmLjC12",
        "colab_type": "text"
      },
      "source": [
        "Download the dataset and divide it into text and labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5sO6GVOiH7D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def download_and_read(url):\n",
        "    local_file = url.split('/')[-1]\n",
        "    p = tf.keras.utils.get_file(local_file, url, \n",
        "        extract=True, cache_dir=\".\")\n",
        "    labels, texts = [], []\n",
        "    local_file = os.path.join(\"datasets\", \"SMSSpamCollection\")\n",
        "    with open(local_file, \"r\") as fin:\n",
        "        for line in fin:\n",
        "            label, text = line.strip().split('\\t')\n",
        "            labels.append(1 if label == \"spam\" else 0)\n",
        "            texts.append(text)\n",
        "    return texts, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiBSCfrFjJwk",
        "colab_type": "text"
      },
      "source": [
        "Building the embedding matrix for our vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJtl3Ed7jVVf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_embedding_matrix(sequences, word2idx, embedding_dim, \n",
        "        embedding_file):\n",
        "    if os.path.exists(embedding_file):\n",
        "        E = np.load(embedding_file)\n",
        "    else:\n",
        "        vocab_size = len(word2idx)\n",
        "        E = np.zeros((vocab_size, embedding_dim))\n",
        "        word_vectors = api.load(EMBEDDING_MODEL)\n",
        "        for word, idx in word2idx.items():\n",
        "            try:\n",
        "                E[idx] = word_vectors.word_vec(word)\n",
        "            except KeyError:   # word not in embedding\n",
        "                pass\n",
        "            # except IndexError: # UNKs are mapped to seq over VOCAB_SIZE as well as 1\n",
        "            #     pass\n",
        "        np.save(embedding_file, E)\n",
        "    return E"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHQPDC7GjOHP",
        "colab_type": "text"
      },
      "source": [
        "Model created using model subclassing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXjIJqSPjusd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SpamClassifierModel(tf.keras.Model):\n",
        "    def __init__(self, vocab_sz, embed_sz, input_length,\n",
        "            num_filters, kernel_sz, output_sz, \n",
        "            run_mode, embedding_weights, \n",
        "            **kwargs):\n",
        "        super(SpamClassifierModel, self).__init__(**kwargs)\n",
        "        if run_mode == \"scratch\":\n",
        "            self.embedding = tf.keras.layers.Embedding(vocab_sz, \n",
        "                embed_sz,\n",
        "                input_length=input_length,\n",
        "                trainable=True)\n",
        "        elif run_mode == \"vectorizer\":\n",
        "            self.embedding = tf.keras.layers.Embedding(vocab_sz, \n",
        "                embed_sz,\n",
        "                input_length=input_length,\n",
        "                weights=[embedding_weights],\n",
        "                trainable=False)\n",
        "        else:\n",
        "            self.embedding = tf.keras.layers.Embedding(vocab_sz, \n",
        "                embed_sz,\n",
        "                input_length=input_length,\n",
        "                weights=[embedding_weights],\n",
        "                trainable=True)\n",
        "        self.dropout = tf.keras.layers.SpatialDropout1D(0.2)\n",
        "        self.conv = tf.keras.layers.Conv1D(filters=num_filters,\n",
        "            kernel_size=kernel_sz,\n",
        "            activation=\"relu\")\n",
        "        self.pool = tf.keras.layers.GlobalMaxPooling1D()\n",
        "        self.dense = tf.keras.layers.Dense(output_sz, \n",
        "            activation=\"softmax\"\n",
        "        )\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.conv(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.dense(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMtmeZxzjSCu",
        "colab_type": "text"
      },
      "source": [
        "some variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vQhDFLnlcqp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_DIR = \"data\"\n",
        "EMBEDDING_NUMPY_FILE = os.path.join(DATA_DIR, \"E.npy\")\n",
        "DATASET_URL = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\"\n",
        "EMBEDDING_MODEL = \"glove-wiki-gigaword-300\"\n",
        "EMBEDDING_DIM = 300\n",
        "NUM_CLASSES = 2\n",
        "BATCH_SIZE = 128\n",
        "NUM_EPOCHS = 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25EEfzr0jWSx",
        "colab_type": "text"
      },
      "source": [
        "reading data into text and labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fl3FtOpImowv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "texts, labels = download_and_read(DATASET_URL)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFE2DYz2UQUh",
        "colab_type": "text"
      },
      "source": [
        "#Embedding matrix models\n",
        "* Word2Vec: Two flavors, one trained on Google news (3 million word vectors based on 3 billion tokens), and one trained on Russian corpora (word2vec-ruscorpora-300, word2vec-google-news-300).\n",
        "\n",
        "* GloVe: Two flavors, one trained on the Gigawords corpus (400,000 word vectors based on 6 billion tokens), available as 50d, 100d, 200d, and 300d vectors, and one trained on Twitter (1.2 million word vectors based on 27 billion tokens), available as 25d, 50d, 100d, and 200d vectors (glove-wiki-gigaword-50, glove-wiki-gigaword-100, glove-wiki-gigaword-200, glove-wiki-gigaword-300, glove-twitter-25, glove-twitter-50, glove-twitter-100, glove-twitter-200).\n",
        "\n",
        "* fastText: 1 million word vectors trained with subword information on Wikipedia 2017, the UMBC web corpus, and statmt.org news dataset (16B tokens). (fastText-wiki-news-subwords-300).\n",
        "\n",
        "* ConceptNet Numberbatch: An ensemble embedding that uses the ConceptNet semantic network, the paraphrase database (PPDB), Word2Vec, and GloVe as input. Produces 600d vectors [12, 13]. (conceptnet-numberbatch-17-06-300)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAdjG4O3jauQ",
        "colab_type": "text"
      },
      "source": [
        "Creating the data folder "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZZsVl5nsHLi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCcW46k_jdsA",
        "colab_type": "text"
      },
      "source": [
        "tokenizing the texts and making all the texts same size using pad sequences and maxsqlen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8HAbHlnn6u-",
        "colab_type": "code",
        "outputId": "e9851942-6302-4d5c-89fd-89704ce1073c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "text_sequences = tokenizer.texts_to_sequences(texts)\n",
        "text_sequences = tf.keras.preprocessing.sequence.pad_sequences(text_sequences)\n",
        "num_records = len(text_sequences)\n",
        "max_seqlen = len(text_sequences[0])\n",
        "print(\"{:d} sentences, max length: {:d}\".format(num_records, max_seqlen))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5574 sentences, max length: 189\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46k0TCB1jm_K",
        "colab_type": "text"
      },
      "source": [
        "encoding labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hb1RTX35sCEn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cat_labels = tf.keras.utils.to_categorical(labels, num_classes=NUM_CLASSES)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuVRKrO7jqGe",
        "colab_type": "text"
      },
      "source": [
        "Creating the dict for word2idx and idx2word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veKvhdl3tIbA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5cc25126-848d-41f1-c154-4eeff278fe62"
      },
      "source": [
        "word2idx = tokenizer.word_index\n",
        "idx2word = {v:k for k, v in word2idx.items()}\n",
        "word2idx[\"PAD\"] = 0\n",
        "idx2word[0] = \"PAD\"\n",
        "vocab_size = len(word2idx)\n",
        "print(\"vocab size: {:d}\".format(vocab_size))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab size: 9010\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ug6byK9SjuGz",
        "colab_type": "text"
      },
      "source": [
        "creating the tf.data.Dataset object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPhNqeRs1N7Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((text_sequences, cat_labels))\n",
        "dataset = dataset.shuffle(10000)\n",
        "test_size = num_records // 4\n",
        "val_size = (num_records - test_size) // 10\n",
        "test_dataset = dataset.take(test_size)\n",
        "val_dataset = dataset.skip(test_size).take(val_size)\n",
        "train_dataset = dataset.skip(test_size + val_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtZKLFowjyi0",
        "colab_type": "text"
      },
      "source": [
        "Making them into batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2TYgz4b3rfS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sjZ8fDpj06S",
        "colab_type": "text"
      },
      "source": [
        "Building the embedding matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMLr0LVpZcAW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "66e1ca56-6b91-4ec3-8fab-99566e58ae0b"
      },
      "source": [
        "E = build_embedding_matrix(text_sequences, word2idx, EMBEDDING_DIM,\n",
        "    EMBEDDING_NUMPY_FILE)\n",
        "print(\"Embedding matrix:\", E.shape)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Embedding matrix: (9010, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_1uk2qfj3wU",
        "colab_type": "text"
      },
      "source": [
        "Instantiating the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iakU_2rDZeUd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "outputId": "fbd54f0f-7455-4bb2-c919-8eb3e1e3ca18"
      },
      "source": [
        "conv_num_filters = 256\n",
        "conv_kernel_size = 3\n",
        "model = SpamClassifierModel(\n",
        "    vocab_size, EMBEDDING_DIM, max_seqlen, \n",
        "    conv_num_filters, conv_kernel_size, NUM_CLASSES,\n",
        "    run_mode, E)\n",
        "model.build(input_shape=(None, max_seqlen))\n",
        "model.summary()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"spam_classifier_model_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      multiple                  2703000   \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d_3 (Spatial multiple                  0         \n",
            "_________________________________________________________________\n",
            "conv1d_3 (Conv1D)            multiple                  230656    \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_3 (Glob multiple                  0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              multiple                  514       \n",
            "=================================================================\n",
            "Total params: 2,934,170\n",
            "Trainable params: 231,170\n",
            "Non-trainable params: 2,703,000\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7InX98BDj6pM",
        "colab_type": "text"
      },
      "source": [
        "compiling the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NuzrzlrZghG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtSV8ZDvj9V3",
        "colab_type": "text"
      },
      "source": [
        "Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZviOHFsZkHY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "outputId": "a5fb613f-028e-4a3e-bfd9-ea18c2432264"
      },
      "source": [
        "model.fit(train_dataset, epochs=NUM_EPOCHS, \n",
        "    validation_data=val_dataset,\n",
        "    class_weight=CLASS_WEIGHTS)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "29/29 [==============================] - 13s 456ms/step - loss: 0.6041 - accuracy: 0.8413 - val_loss: 0.1141 - val_accuracy: 0.9583\n",
            "Epoch 2/3\n",
            "29/29 [==============================] - 13s 450ms/step - loss: 0.2509 - accuracy: 0.9531 - val_loss: 0.0604 - val_accuracy: 0.9844\n",
            "Epoch 3/3\n",
            "29/29 [==============================] - 13s 450ms/step - loss: 0.1576 - accuracy: 0.9714 - val_loss: 0.1008 - val_accuracy: 0.9661\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f0967d733c8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9JCZ7pxkB3J",
        "colab_type": "text"
      },
      "source": [
        "Converting dataset object into predictions list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WlLYNJzZm1P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels, predictions = [], []\n",
        "for Xtest, Ytest in test_dataset:\n",
        "    Ytest_ = model.predict_on_batch(Xtest)\n",
        "    ytest = np.argmax(Ytest, axis=1)\n",
        "    ytest_ = np.argmax(Ytest_, axis=1)\n",
        "    labels.extend(ytest.tolist())\n",
        "    predictions.extend(ytest.tolist())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAZ8jNehkK7A",
        "colab_type": "text"
      },
      "source": [
        "confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_WI1S-oZwWT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "81e1140e-aeca-4c70-ea70-f92fec229e49"
      },
      "source": [
        "print(\"test accuracy: {:.3f}\".format(accuracy_score(labels, predictions)))\n",
        "print(\"confusion matrix\")\n",
        "print(confusion_matrix(labels, predictions))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test accuracy: 1.000\n",
            "confusion matrix\n",
            "[[1091    0]\n",
            " [   0  189]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giur3axjkN6U",
        "colab_type": "text"
      },
      "source": [
        "As we can we got 100% accuracy on the test set"
      ]
    }
  ]
}